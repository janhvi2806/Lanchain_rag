{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595082a1-526b-4c96-afe2-9fec2ca2924f",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) with open-source Hugging Face LLMs using LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e6fb0d-db38-46df-9b2b-a7b5058c4dac",
   "metadata": {},
   "source": [
    "![RAG pic](images/RAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf48ed-bb4f-481e-8051-7c4b2dbceee8",
   "metadata": {},
   "source": [
    "## Introduction: \n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is an approach in natural language processing (NLP) that enhances the capabilities of generative models by integrating external knowledge retrieval into the generation process. This technique aims to improve the quality, relevance, and factual accuracy of the generated text by allowing the model to dynamically access and incorporate information from a large corpus of documents or databases during the generation task. The process involves two key components: a retrieval system and a generative model.\n",
    "\n",
    "**Working Mechanism**\n",
    "\n",
    "The working mechanism of RAG typically involves the following steps:\n",
    "\n",
    "- Query Formation: The system formulates a query based on the initial input or prompt. This query is designed to retrieve information that is likely to be relevant to generating the desired output.\n",
    "\n",
    "- Information Retrieval: The formulated query is used to fetch relevant information from an external database or knowledge base. The retrieval system may return one or more documents, passages, or data entries that match the query.\n",
    "\n",
    "- Content Integration: The retrieved information, along with the original input, is provided to the generative model. The model then integrates this information to produce a coherent and contextually enriched output.\n",
    "\n",
    "- Generation: The generative model synthesizes the final text, taking into account both the input and the retrieved external information. This step ensures that the output is not only relevant and informative but also maintains a natural and fluent language style.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5260f0f7-e8a5-4051-9aed-42c7729ac92d",
   "metadata": {},
   "source": [
    "## Library installation\n",
    "- Create a virtual environment and install the necessary python libraries\n",
    "- `pip install transformers sentence-tranformers langchain torch faiss-cpu numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf1fe1-d539-4099-8967-5ce05c2be2a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Library configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd02f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: langchain_community in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (0.3.14)\n",
      "Requirement already satisfied: langchain in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (0.3.14)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain_community) (2.7.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain_community) (3.11.11)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain_community) (0.2.10)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain_community) (0.3.29)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain_community) (2.0.36)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain) (2.10.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_community) (24.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain_community) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain_community) (3.10.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.0.7)\n",
      "Requirement already satisfied: anyio in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (4.7.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\projects\\lanchain_rag\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Projects\\Lanchain_rag\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy langchain_community langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "280bbd1d-5b03-403c-989a-fe44ec3efc27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b1dd5-d9b4-466c-ad18-8adb9e785470",
   "metadata": {},
   "source": [
    "## Document preparation\n",
    "**We are going to download 4 publications from United States Census Bureau on the following topics:**\n",
    "- Occupation, Earnings, and Job Characteristics: July 2022\n",
    "- Household Income in States andMetropolitan Areas: 2022\n",
    "- Poverty in States and Metropolitan Areas: 2022\n",
    "- Health Insurance Coverage Status and Type by Geography: 2021 and 2022\n",
    "\n",
    "We prepare this documents for the LLM to use as a knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da88ba1c-1f1f-400f-bc28-b9bd60a1e6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download documents from U.S. Census Bureau to local directory.\n",
    "os.makedirs(\"us_census\", exist_ok=True)\n",
    "files = [\n",
    "    \"https://www.census.gov/content/dam/Census/library/publications/2022/demo/p70-178.pdf\",\n",
    "    \"https://www.census.gov/content/dam/Census/library/publications/2023/acs/acsbr-017.pdf\",\n",
    "    \"https://www.census.gov/content/dam/Census/library/publications/2023/acs/acsbr-016.pdf\",\n",
    "    \"https://www.census.gov/content/dam/Census/library/publications/2023/acs/acsbr-015.pdf\",\n",
    "]\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"us_census\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fca755-2b10-4cef-a084-fc1596697f7f",
   "metadata": {},
   "source": [
    "**Split documents to smaller chunks** \n",
    "\n",
    "Documents should be: \n",
    "- large enough to contain enough information to answer a question, and \n",
    "- small enough to fit into the LLM prompt: Mistral-7B-v0.1 input tokens limited to 4096 tokens\n",
    "- small enough to fit into the embeddings model: BAAI/bge-small-en-v1.5: input tokens limited to 512 tokens (roughly 2000 characters. Note: 1 token ~ 4 characters).\n",
    "\n",
    "For this project, we are going to split documents to chunks of roughly 700 characters with an overlap of 50 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b58c08-9077-4f68-9dd6-0333b5bb10a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pdf files in the local directory\n",
    "loader = PyPDFDirectoryLoader(\"./us_census/\")\n",
    "\n",
    "docs_before_split = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap  = 50,\n",
    ")\n",
    "docs_after_split = text_splitter.split_documents(docs_before_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11a38354-0612-42ba-bde3-0cfa4514f540",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'us_census\\\\acsbr-015.pdf', 'page': 0}, page_content='Health Insurance Coverage Status and Type \\nby Geography: 2021 and 2022\\nAmerican Community Survey Briefs\\nACSBR-015\\nIssued September 2023\\nDouglas Conway and Breauna Branch\\nINTRODUCTION\\nDemographic shifts as well as economic and govern-\\nment policy changes can affect people’s access to \\nhealth coverage. For example, between 2021 and 2022, \\nthe labor market continued to improve, which may \\nhave affected private coverage in the United States \\nduring that time.1 Public policy changes included \\nthe renewal of the Public Health Emergency, which \\nallowed Medicaid enrollees to remain covered under \\nthe Continuous Enrollment Provision.2 The American \\nRescue Plan (ARP) enhanced Marketplace premium')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_after_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79aeb73e-5c39-46b6-b144-395ade93c366",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before split, there were 63 documents loaded, with average characters equal to 3840.\n",
      "After split, there were 398 documents (chunks), with average characters equal to 624 (average chunk length).\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda docs: sum([len(doc.page_content) for doc in docs])//len(docs)\n",
    "avg_char_before_split = avg_doc_length(docs_before_split)\n",
    "avg_char_after_split = avg_doc_length(docs_after_split)\n",
    "\n",
    "print(f'Before split, there were {len(docs_before_split)} documents loaded, with average characters equal to {avg_char_before_split}.')\n",
    "print(f'After split, there were {len(docs_after_split)} documents (chunks), with average characters equal to {avg_char_after_split} (average chunk length).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b957ff3-df1a-4e75-9192-1b9c8c85927b",
   "metadata": {},
   "source": [
    "## Text Embeddings with Hugging Face Embedding Models\n",
    "At the time of writing, there are 213 text embeddings models for English on the [Massive Text Embedding Benchmark (MTEB) leaderboard](https://huggingface.co/spaces/mteb/leaderboard). For our project, we are using LangChain's HuggingFaceBgeEmbeddings (BGE models on the Hugging Face), which according to LangChain are \"the best open-source embedding models\". Currently, **BAAI/bge-small-en-v1.5** model is the 26th on MTEB leaderboard with max tokens: 512 tokens, embedding dimensions: 384 and model size: 0.13GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cbff637-d938-445d-b769-a2122ded10f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSUSERSL123\\AppData\\Local\\Temp\\ipykernel_12596\\2508658439.py:1: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
      "C:\\Projects\\Lanchain_rag\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",  # alternatively use \"sentence-transformers/all-MiniLM-l6-v2\" for a light and faster experience.\n",
    "    model_kwargs={'device':'cpu'}, \n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98862ede-96af-4d76-ba54-758eecc11932",
   "metadata": {},
   "source": [
    "Now we can see how a sample embedding would look like for one of those chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b35b55a-3e96-4352-bf5a-5156bb1b5fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embedding of a document chunk:  [-7.50839338e-02 -1.18847201e-02 -3.14887799e-02  2.94038393e-02\n",
      "  5.03487326e-02  5.62426373e-02 -1.69078354e-02  3.46887931e-02\n",
      " -9.79061648e-02 -2.52804812e-02  7.62901455e-02  5.73798455e-02\n",
      " -2.42906548e-02 -3.06789018e-02  6.20469963e-03  4.02187146e-02\n",
      " -8.71848594e-03 -8.25359207e-03 -3.58134173e-02  3.61375995e-02\n",
      " -4.81582694e-02  4.19039950e-02 -3.68844010e-02 -5.38902171e-02\n",
      "  1.65312402e-02  1.20014017e-02 -1.46692069e-02  2.12824382e-02\n",
      " -5.34791127e-02 -1.49481446e-01  2.32285121e-03  3.20234448e-02\n",
      " -5.21530136e-02 -2.28869710e-02  2.32763197e-02  2.14959085e-02\n",
      " -1.39636965e-02  7.53694996e-02  5.07269725e-02  5.50112948e-02\n",
      " -3.30469646e-02  1.79151166e-02 -2.08278149e-02  1.26921723e-03\n",
      " -2.61849463e-02  2.41152430e-03 -1.92517750e-02  3.06515419e-03\n",
      "  1.50291016e-03 -5.14435843e-02  3.78552340e-02 -1.38906902e-02\n",
      "  4.22979221e-02  6.64815381e-02  6.62789792e-02 -4.27095369e-02\n",
      "  9.69380699e-03 -3.63818668e-02 -4.47576679e-02  3.42478417e-02\n",
      "  4.80892286e-02 -8.17981362e-03 -2.61432797e-01  9.35042277e-02\n",
      " -1.23177711e-02  4.97916006e-02 -1.26947248e-02 -6.46239147e-03\n",
      " -1.85768828e-02 -4.12084982e-02 -6.70618042e-02  4.55933101e-02\n",
      " -5.92866987e-02  3.21131898e-03  4.57437485e-02  3.09345294e-02\n",
      "  7.64067145e-03  8.84656142e-03  2.94093844e-02 -2.58820830e-03\n",
      "  2.92377789e-02  2.66136862e-02 -4.13992349e-03 -5.68609461e-02\n",
      "  4.24529836e-02 -9.15010124e-02  4.89868000e-02 -3.28428783e-02\n",
      "  2.88802814e-02 -2.54161358e-02 -3.97303812e-02  6.99066371e-03\n",
      "  8.30586068e-03  3.97988856e-02  1.46019962e-02  3.49585488e-02\n",
      "  2.64397566e-03 -1.07523217e-03 -2.48149745e-02  3.34885657e-01\n",
      " -2.84330361e-02  1.63665246e-02 -1.49358502e-02  2.00819597e-02\n",
      " -1.20456368e-02 -5.32911681e-02 -7.13262893e-03  1.95750315e-03\n",
      "  2.21331492e-02  1.57228298e-02  1.40003609e-02 -2.88736466e-02\n",
      "  3.15910615e-02  4.62217927e-02 -3.93753573e-02 -1.17808031e-02\n",
      "  3.67968306e-02  1.82423368e-02  1.04911834e-01 -2.51977928e-02\n",
      " -7.72676617e-03  2.85531115e-02  2.39119632e-03 -4.62964214e-02\n",
      "  2.06293669e-02  6.91121966e-02  5.51006347e-02  1.45288542e-01\n",
      "  2.00893786e-02 -4.90959771e-02  9.72043797e-02 -3.14234234e-02\n",
      " -6.82204589e-03  8.70185718e-03  7.88772106e-03  1.01440763e-02\n",
      " -2.98855044e-02  3.83801870e-02 -4.76639834e-04  4.60920855e-02\n",
      "  4.26522223e-03  1.92257366e-03  3.72980186e-03 -1.43844903e-01\n",
      " -3.14254165e-02  1.84120327e-01 -2.66538206e-02  3.93033028e-02\n",
      "  2.71653384e-02 -9.13588051e-03 -4.36423309e-02  4.68981639e-02\n",
      " -2.24493127e-02  3.65314260e-02 -3.26293297e-02  2.54269019e-02\n",
      "  8.98860302e-03  2.21870467e-02 -2.53295004e-02 -6.52057305e-02\n",
      "  5.96357584e-02 -3.80218849e-02 -6.74996376e-02  2.13768072e-02\n",
      "  3.28018777e-02 -3.56060937e-02 -7.09049730e-03 -6.29472286e-02\n",
      "  3.46652754e-02  2.94755190e-03  1.10380827e-02  4.49174233e-02\n",
      "  1.85897965e-02 -3.57448757e-02  9.84327719e-02  1.36011187e-02\n",
      " -2.42052823e-02 -8.81964341e-03 -1.90354902e-02 -6.89528584e-02\n",
      "  1.13090621e-02 -2.34223921e-02 -5.76472096e-02 -3.99872437e-02\n",
      " -1.80143919e-02 -4.59445566e-02 -4.32488099e-02  5.04179820e-02\n",
      "  5.98202907e-02 -1.24294693e-02 -6.36572763e-03  9.22003947e-03\n",
      " -6.10286705e-02  3.51958983e-02 -8.42589512e-03 -6.96953479e-03\n",
      " -5.43499887e-02 -1.61404889e-02  3.91421840e-02 -3.27162594e-02\n",
      " -3.36360112e-02  3.14964056e-02  1.46583943e-02 -3.08335875e-03\n",
      " -1.00682080e-02  2.54443474e-02  2.90144738e-02 -6.02994002e-02\n",
      "  3.71564925e-02 -7.00379489e-03  2.12549581e-03  4.51739281e-02\n",
      "  3.97064649e-02  1.13278849e-03  2.60367338e-02  2.39327550e-02\n",
      "  1.21051567e-02 -1.98410284e-02  2.89613428e-03  3.52065675e-02\n",
      "  3.06326095e-02  7.55703822e-02  8.69498253e-02 -2.71886826e-01\n",
      " -1.01404153e-02  7.39601441e-03  7.15576857e-03 -6.00429252e-02\n",
      " -5.37454225e-02 -4.08391915e-02  3.60599011e-02  1.45023623e-02\n",
      "  8.61998498e-02  6.04105741e-02  3.75893223e-03 -2.33943593e-02\n",
      "  8.01002979e-02  8.06647912e-03 -7.77082220e-02  2.96195429e-02\n",
      " -3.67732607e-02 -1.51859047e-02  5.20401495e-03  2.32015795e-04\n",
      "  2.84862844e-03 -7.65166506e-02 -1.83249377e-02  7.28286132e-02\n",
      " -1.51562365e-02  6.44024089e-02 -5.31626455e-02 -6.88023716e-02\n",
      " -1.01751285e-02 -5.86170107e-02  5.46874404e-02 -1.30900303e-02\n",
      " -1.14680119e-01  5.81667908e-02  2.51455680e-02 -8.56098756e-02\n",
      "  2.26653460e-03 -5.81913069e-02 -3.33960950e-02 -2.49850787e-02\n",
      "  4.69970927e-02 -6.48618415e-02  2.31449325e-02 -2.60249507e-02\n",
      " -6.57254383e-02  4.26158607e-02  7.96896368e-02 -7.34064654e-02\n",
      "  1.32927326e-02  4.74229176e-03 -2.38549020e-02 -2.07158308e-02\n",
      " -5.30740852e-03  2.43275370e-02 -6.51929006e-02 -2.45433729e-02\n",
      "  3.93315367e-02 -1.75772738e-02 -2.06312095e-03  2.85326801e-02\n",
      "  1.58546157e-02  3.48942466e-02 -9.82272718e-03 -6.95983320e-03\n",
      " -5.52597381e-02 -4.58818569e-04 -2.82527804e-02 -7.72426799e-02\n",
      "  8.46334547e-03  4.14262433e-03  5.53764068e-02 -2.14621276e-02\n",
      " -2.43417323e-02  1.66745503e-02 -1.66574400e-02  4.91909534e-02\n",
      "  2.13858322e-03 -4.86749317e-03 -5.13635390e-02  4.25720587e-02\n",
      " -5.67146093e-02  1.93783399e-02  4.02168408e-02 -8.94284528e-03\n",
      "  1.87669620e-02  2.86187325e-03  3.87538900e-03 -1.42965456e-02\n",
      "  1.85692292e-02 -2.04248987e-02 -1.69339534e-02 -2.08898447e-02\n",
      " -3.33977379e-02 -2.28457176e-03 -3.34119587e-03 -2.17555821e-01\n",
      "  4.97976542e-02  3.50592583e-02 -1.57288313e-02 -1.56817771e-02\n",
      " -7.38151511e-03 -3.35963629e-02  7.42068002e-03 -6.02514204e-03\n",
      " -2.71219406e-02  4.86272983e-02  8.54909346e-02  1.23634174e-01\n",
      " -1.70354247e-02 -1.62392817e-02  2.80817202e-03  7.15028644e-02\n",
      "  1.67272345e-03  1.61709767e-02 -2.90143173e-02  5.38625866e-02\n",
      " -7.07101375e-02  1.52645648e-01 -3.06274090e-02  2.72723734e-02\n",
      " -7.34533966e-02 -6.98406063e-03  4.07376625e-02 -2.78192759e-02\n",
      "  1.46546988e-02  1.85798034e-02 -3.86150135e-03  3.50741893e-02\n",
      " -2.81762145e-02  5.29079251e-02  3.09830438e-03  8.15661624e-03\n",
      "  5.76677322e-02  1.20558385e-02  1.60512757e-02 -3.98612656e-02\n",
      " -2.24486068e-02  1.00878356e-02  7.98118673e-03  6.20987527e-02\n",
      " -2.58468706e-02 -5.01606055e-02 -4.76673283e-02 -3.93508747e-03\n",
      "  6.24541156e-02  4.03239904e-03 -1.77497473e-02 -9.33069363e-03\n",
      "  1.10814208e-02 -5.74615002e-02 -1.47054875e-02 -5.08489944e-02\n",
      "  3.05494573e-02 -5.38100721e-03 -4.82051447e-03  2.84002293e-02\n",
      "  3.56259868e-02 -7.45407492e-02  1.83500778e-02  6.66981861e-02]\n",
      "Size of the embedding:  (384,)\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = np.array(huggingface_embeddings.embed_query(docs_after_split[0].page_content))\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77031a88-063e-40b7-b543-982e47b037a4",
   "metadata": {},
   "source": [
    "## Retrieval System for vector embeddings\n",
    "Once we have a embedding model, we are ready to vectorize all our documents and store them in a vector store to construct a retrieval system. With specifically designed searching algorithms, a retrieval system can do similarity searching efficiently to retrieve relevant documents.\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other. It solves limitations of traditional query search engines that are optimized for hash-based searches, and provides more scalable similarity search functions (nearest-neighbor search implementations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efb25e38-750d-490d-be4a-c15e6d789167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs_after_split, huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "065ce4c9-d269-4c4d-ba44-1856ffaf240f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 documents retrieved which are relevant to the query. Display the first one:\n",
      "\n",
      "Comparisons\n",
      "The U.S. median household income \n",
      "in 2022 was $74,755, according \n",
      "Figure 1.\n",
      "Median Household Income in the Past 12 Months in the United States: 2005–2022\n",
      " \n",
      "Note: Estimates for 2020 experimental data not shown. For more information on the 2020 experimental data products, \n",
      "refer to <www.census.gov/programs-surveys/acs/technical-documentation/user-notes/2021-02.html>. Information on \n",
      "conﬁdentiality protection, sampling error, nonsampling error, and deﬁnitions is available at <www.census.gov/acs>.\n",
      "Source: U.S. Census Bureau, 2005–2022 American Community Survey, 1-year estimates.\n",
      "Recession\n",
      "/zero.tab\n",
      "/five.tab/five.tab\n",
      "/six.tab/zero.tab\n",
      "/six.tab/five.tab\n",
      "/seven.tab/zero.tab\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"What were the trends in median household income across different states in the United States between 2021 and 2022.\"\"\"  # Sample question, change to other questions you are interested in.\n",
    "relevant_documents = vectorstore.similarity_search(query)\n",
    "print(f'There are {len(relevant_documents)} documents retrieved which are relevant to the query. Display the first one:\\n')\n",
    "print(relevant_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105646d7-b417-41d2-b981-48cae9e20496",
   "metadata": {},
   "source": [
    "### Create a retriever interface using vector store, we'll use it later to construct Q & A chain using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15c6b860-1146-4256-8aad-5dd5cc245f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use similarity searching algorithm and return 3 most relevant documents.\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623c760-f5b5-4a07-b303-d33f195b0746",
   "metadata": {},
   "source": [
    "**Now we have our vector store and retrieval system ready. We then need a large language model (LLM) to process information and answer the question.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a8300b-3a58-4026-bf1c-b18ebc7c9c15",
   "metadata": {},
   "source": [
    "## Open-source LLMs from Hugging Face\n",
    "**There two ways to utilize Hugging Face LLMs: online and local.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b62e27-1ce6-41c4-b25e-c154c7cf3cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hugging Face Hub\n",
    "The Hugging Face Hub is an platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. \n",
    "\n",
    "- To use, we should have the huggingface_hub python package installed.\n",
    "- Set an environment variable called HUGGINGFACEHUB_API_TOKEN with your Hugging Face access token in it.\n",
    "- Currently, HuggingFace LangChain integration doesn't support the question-answering task, so we can't select HuggingFace QA models for this project. Instead, we select LLMs from the text-generation task category.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cefbd53a-51c1-4c1a-a05c-b9b0aaa1593a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "# hf = HuggingFaceHub(\n",
    "#     repo_id=\"EleutherAI/gpt-neo-2.7B\",\n",
    "#     model_kwargs={\"temperature\":0.1, \"max_length\":500})\n",
    "\n",
    "#query = \"\"\"What were the trends in median household income across different states in the United States between 2021 and 2022.\"\"\"  # Sample question, change to other questions you are interested in.\n",
    "# hf.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ed6e2c-10cc-4c07-a0d9-97386dfcd6f2",
   "metadata": {},
   "source": [
    "Hugging Face Hub will be slow when you run large models. You can get around this by downloading the model and run it on your local machine. This is the way we use LLM in our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62f871-4ee2-4296-9a81-53f7eb95f822",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hugging Face Local Pipelines\n",
    "\n",
    "Hugging Face models can be run locally through the HuggingFacePipeline class.\n",
    "\n",
    "- We need to install transformers python package.\n",
    "- The Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters. Mistral-7B-v0.1 outperforms Llama-2-13B on all benchmarks tested. Read the [paper](https://arxiv.org/abs/2310.06825).\n",
    "- Mistral-7B-v0.1's model size is 3.5GB, while Llama-2–13B has 13 billion parameters and 25GB model size.\n",
    "- In order to use Llama2, you need to request access from Meta. Mistral-7B-v0.1 is publicly available already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30285364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 documents retrieved which are relevant to the query. Display the first one:\n",
      "\n",
      "Comparisons\n",
      "The U.S. median household income \n",
      "in 2022 was $74,755, according \n",
      "Figure 1.\n",
      "Median Household Income in the Past 12 Months in the United States: 2005–2022\n",
      " \n",
      "Note: Estimates for 2020 experimental data not shown. For more information on the 2020 experimental data products, \n",
      "refer to <www.census.gov/programs-surveys/acs/technical-documentation/user-notes/2021-02.html>. Information on \n",
      "conﬁdentiality protection, sampling error, nonsampling error, and deﬁnitions is available at <www.census.gov/acs>.\n",
      "Source: U.S. Census Bureau, 2005–2022 American Community Survey, 1-year estimates.\n",
      "Recession\n",
      "/zero.tab\n",
      "/five.tab/five.tab\n",
      "/six.tab/zero.tab\n",
      "/six.tab/five.tab\n",
      "/seven.tab/zero.tab\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"What were the trends in median household income across different states in the United States between 2021 and 2022.\"\"\"  # Sample question, change to other questions you are interested in.\n",
    "relevant_documents = vectorstore.similarity_search(query)\n",
    "print(f'There are {len(relevant_documents)} documents retrieved which are relevant to the query. Display the first one:\\n')\n",
    "print(relevant_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9a472-a34d-418d-a129-971b987fb023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    # model_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_id=\"Mistral-7B-v0.1\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"temperature\": 0, \"max_new_tokens\": 300}\n",
    ")\n",
    "\n",
    "llm = hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab280be3-8b48-467e-80bf-1605ff17d616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e73a4eb-19b1-4b45-a96b-83290d1921a9",
   "metadata": {},
   "source": [
    "**At a glance, our LLM generates some output that might seem plausible but not accurate or factual. That is because it has not been trained on the census data of recent years.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee0794-fb8c-43e6-bac1-52bd3e8511f1",
   "metadata": {},
   "source": [
    "- OpenAI GPT-3.5 model (for test purpose only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81428990-041a-466d-8a98-29423290a0bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# chat = ChatOpenAI(temperature=0)\n",
    "# chat.invoke(query)\n",
    "# llm = chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67737c0-66f2-447b-b38b-f7dd63794110",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Q & A chain \n",
    "Now we have both the retrieval system for relevant documents and LLM as QA chatbot ready.\n",
    "\n",
    "We will take our initial query, together with the relevant documents retrieved based on the results of our similarity search, to create a prompt to feed into the LLM. The LLM will take the initial query as the question and relevant documents as the context information to generate a result.\n",
    "\n",
    "Luckily, **LangChain** provides an abstraction of the whole pipeline - **RetrievalQA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1814450-da39-4958-bc9a-8f9ce3c99acf",
   "metadata": {},
   "source": [
    "**Let's first construct a proper prompt for our task.**\n",
    "\n",
    "Prompt engineering is another crucial factor in LLM's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c55f5c61-e016-4c26-9038-b063516835ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer but you may want to check the following links\".\n",
    "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43bc986-93af-49a6-84cc-eda2456f864f",
   "metadata": {},
   "source": [
    "Call LangChain's RetrievalQA with the prompt above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72e1a9bb-7ecb-475a-a7f7-ca974a8a77a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c40cd5-6125-4d9b-8c2c-5a6a31497d4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use RetrievalQA invoke method to execute the chain\n",
    "Note that Input of [invoke method](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html#langchain.chains.retrieval_qa.base.RetrievalQA.invoke) needs to be a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f84ff6-7869-44ca-8310-90adec711b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the QA chain with our query.\n",
    "result = retrievalQA.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba61d2-ea29-4ca4-b94c-cc64232aa972",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86191166-20e2-4b1a-aaa4-f4742542adfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relevant_docs = result['source_documents']\n",
    "print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')\n",
    "print(\"*\" * 100)\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"Relevant Document #{i+1}:\\nSource file: {doc.metadata['source']}, Page: {doc.metadata['page']}\\nContent: {doc.page_content}\")\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92070ecc-452b-4ef8-a3e7-4b5d13cd548d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Enhanced Accuracy and Relevance: By leveraging external sources, RAG models can generate content that is more accurate, detailed, and relevant to the given context.\n",
    "- Factuality: It helps in improving the factuality of the generated text, as the information is directly sourced from reliable external databases or knowledge bases.\n",
    "- Versatility: RAG can be applied to a wide range of NLP tasks, including question answering, text summarization, content creation, and more, enhancing their performance by providing access to a broader range of information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
